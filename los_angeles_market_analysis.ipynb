{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise do Mercado de Cafeterias com Garçons Robôs em Los Angeles\n",
    "\n",
    "O projeto consiste em abrir uma pequena cafeteria com garçons robôs em Los Angeles. Com o objetivo de atrair investidores, a equipe decidiu realizar uma pesquisa de mercado para analisar as condições atuais do setor e determinar a viabilidade e sustentabilidade do negócio a longo prazo, considerando o possível esgotamento do interesse inicial na novidade dos garçons robôs.\n",
    "\n",
    "Para realizar essa análise, a equipe conta com um especialista em análise de dados, responsável por reunir e analisar informações relevantes sobre o mercado de cafeterias em Los Angeles. Os dados utilizados são provenientes de fontes de código aberto, fornecendo informações sobre outros restaurantes na região.\n",
    "\n",
    "O objetivo dessa pesquisa de mercado é fornecer uma visão abrangente do setor de cafeterias em Los Angeles, identificando tendências, concorrência e oportunidades. A equipe espera que os resultados obtidos ajudem a tomar decisões estratégicas informadas e a atrair investidores interessados no potencial de sucesso e crescimento sustentável da cafeteria com garçons robôs.\n",
    "\n",
    "Essa análise minuciosa do mercado é essencial para garantir que a cafeteria com garçons robôs seja capaz de se manter competitiva mesmo após a novidade inicial perder o brilho, maximizando suas chances de sucesso e estabelecendo uma posição sólida no setor de alimentação em Los Angeles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução ao Projeto\n",
    "\n",
    "Neste projeto, será realizada uma análise de dados sobre o mercado de restaurantes em Los Angeles. O objetivo é obter insights e recomendações para investidores interessados em abrir um novo estabelecimento na região. Os dados utilizados são provenientes de um conjunto de dados de restaurantes em LA disponíveis em formato CSV.\n",
    "\n",
    "\n",
    "**Passo 1: Carregar e Preparar os Dados**\n",
    "\n",
    "Nesta etapa, os dados sobre restaurantes em LA devem ser carregados e preparados para a análise. Certifique-se de que os tipos de dados estão corretos, não há valores ausentes ou duplicados. Se necessário, realize o processamento dos dados.\n",
    "\n",
    "Caminho do arquivo: /datasets/rest_data_us.csv\n",
    "\n",
    "**Passo 2: Análise de Dados**\n",
    "\n",
    "Nesta etapa, serão realizadas várias análises para entender o mercado de restaurantes em Los Angeles. Algumas das tarefas incluem:\n",
    "\n",
    "- Investigar as proporções de vários tipos de estabelecimentos e de redes versus estabelecimentos independentes.\n",
    "\n",
    "- Identificar o tipo de estabelecimento típico para redes e o que caracteriza as redes em termos de número de estabelecimentos e número de assentos.\n",
    "\n",
    "- Determinar o número médio de assentos para cada tipo de restaurante e identificar o tipo com maior média de assentos.\n",
    "\n",
    "- Analisar os nomes das ruas presentes na coluna \"address\" e identificar as dez ruas com o maior número de restaurantes.\n",
    "\n",
    "- Contar o número de ruas que possuem apenas um restaurante.\n",
    "\n",
    "- Analisar a distribuição do número de assentos em ruas com muitos restaurantes.\n",
    "\n",
    "**Passo 3: Preparar uma Apresentação**\n",
    "\n",
    "Nesta etapa final, será preparada uma apresentação com base na pesquisa realizada. A apresentação deve incluir os principais resultados da análise, conclusões e recomendações para o tipo mais apropriado de restaurante e número de assentos. A apresentação pode ser criada em qualquer ferramenta de sua escolha, mas deverá ser convertida para o formato PDF para avaliação.\n",
    "\n",
    "Link para a apresentação em formato PDF: [Inserir link aqui]\n",
    "\n",
    "Certifique-se de seguir as diretrizes de formatação fornecidas no capítulo \"Preparando Apresentações\" do curso.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação dos nossos dados e carregamento das bases\n",
    "\n",
    "Iremos carregar as bibliotecas que iremos usar para gerar as análises e gráficos.\n",
    "\n",
    "Iremos explicar cada um dos usos das bibliotecas:\n",
    "\n",
    "* pandas --> análise de dados;\n",
    "* numpy --> geração de gráficos e auxílio ao pandas;\n",
    "* matplotlib.pyplot --> geração de gráficos \n",
    "\n",
    "Após isso iremos abrir nossos dados em diferentes seções e analisar a qualidade das bases e realizar as correções devidas em cada uma das seções como pré-análise de dados e processamento de base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting recordlinkage\n",
      "  Downloading recordlinkage-0.16-py3-none-any.whl (926 kB)\n",
      "\u001b[K     |████████████████████████████████| 926 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /Users/gabrielreus/opt/anaconda3/lib/python3.8/site-packages (from recordlinkage) (1.0.1)\n",
      "Requirement already satisfied: jellyfish>=1 in /Users/gabrielreus/opt/anaconda3/lib/python3.8/site-packages (from recordlinkage) (1.0.0)\n",
      "Requirement already satisfied: scipy>=1 in /Users/gabrielreus/opt/anaconda3/lib/python3.8/site-packages (from recordlinkage) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.13 in /Users/gabrielreus/opt/anaconda3/lib/python3.8/site-packages (from recordlinkage) (1.22.4)\n",
      "Collecting scikit-learn>=1\n",
      "  Downloading scikit_learn-1.3.1-cp38-cp38-macosx_10_9_x86_64.whl (10.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1 MB 7.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas<3,>=1 in /Users/gabrielreus/opt/anaconda3/lib/python3.8/site-packages (from recordlinkage) (1.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/gabrielreus/opt/anaconda3/lib/python3.8/site-packages (from pandas<3,>=1->recordlinkage) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/gabrielreus/opt/anaconda3/lib/python3.8/site-packages (from pandas<3,>=1->recordlinkage) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/gabrielreus/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas<3,>=1->recordlinkage) (1.15.0)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[K     |████████████████████████████████| 302 kB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /Users/gabrielreus/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=1->recordlinkage) (2.1.0)\n",
      "Installing collected packages: joblib, scikit-learn, recordlinkage\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.0.1\n",
      "    Uninstalling joblib-1.0.1:\n",
      "      Successfully uninstalled joblib-1.0.1\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.1\n",
      "    Uninstalling scikit-learn-0.24.1:\n",
      "      Successfully uninstalled scikit-learn-0.24.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "auto-sklearn 0.15.0 requires scikit-learn<0.25.0,>=0.24.0, but you have scikit-learn 1.3.1 which is incompatible.\u001b[0m\n",
      "Successfully installed joblib-1.3.2 recordlinkage-0.16 scikit-learn-1.3.1\n"
     ]
    }
   ],
   "source": [
    "# instalando recordlinkage\n",
    "!pip install recordlinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando nossas bibliotecas\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import recordlinkage\n",
    "from recordlinkage.preprocessing import clean\n",
    "import plotly.graph_objects as go\n",
    "import plotly.colors\n",
    "import plotly.figure_factory as ff\n",
    "import seaborn as sns\n",
    "import plotly.subplots as sp\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base rest_data_us.csv e carregamento de dados\n",
    "\n",
    "Como observado anteriormente iremos abrir a base de dados **rest_data_us.csv** realizando uma pré-análise de dados, processamento do dado e possível limpeza, indicando os erros e as mudanças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/datasets/rest_data_us.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e98790d1bafe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdf_rest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/downloads/visits_log_us.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/downloads/visits_log_us.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e98790d1bafe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Carregando dados no JupyterHub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdf_rest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/datasets/rest_data_us.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/datasets/rest_data_us.csv'"
     ]
    }
   ],
   "source": [
    "# abrindo a tabela visits_log_us\n",
    "\n",
    "# Carregando dados no computador pessoal\n",
    "try:\n",
    "    df_rest = pd.read_csv(r'/downloads/visits_log_us.csv')\n",
    "    \n",
    "# Carregando dados no JupyterHub\n",
    "except:\n",
    "    df_rest = pd.read_csv(r'/datasets/rest_data_us.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostrando a df \n",
    "df_rest.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# informações gerais da df\n",
    "df_rest.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão Intermediária**\n",
    "\n",
    "Podemos observar que há dados faltantes para a coluna de Chain. Iremos analisar os dados faltantes para entender o erro. \n",
    "\n",
    "Além disso, iremos verificar as colunas para entender se não existem dados duplicados em id, object_name e address. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<strong>Comentário do revisor v1</strong>\n",
    "\n",
    "Excelente trabalho checando a consistência dos dados.\n",
    "\n",
    "Sempre importante para garantir uma análise robusta!\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpeza de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados duplicados\n",
    "\n",
    "Iremos analisar se existem dados duplicados que recebemos e também duplicatas implícitas.\n",
    "\n",
    "Vamos criar uma Dataframe filtrada somente com dados duplicados para análise mais aprofundada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar os dados duplicados\n",
    "df_duplicates = df_rest[df_rest.duplicated()]\n",
    "\n",
    "# mostrando a df\n",
    "df_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão Intermediária**\n",
    "\n",
    "Não existem dados duplicados gerais. Assim, iremos seguir a análise para duplicatas para cada uma das colunas para entendimento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicadas em id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando df\n",
    "duplicate_id = df_rest[df_rest['id'].duplicated()]\n",
    "\n",
    "# mostrando df\n",
    "duplicate_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão**\n",
    "Não existem dados duplicados para id, podemos seguir para a próxima coluna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicadas em object_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando df\n",
    "duplicate_object = df_rest[df_rest['object_name'].duplicated()]\n",
    "\n",
    "# mostrando df\n",
    "duplicate_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão Intermediária**\n",
    "\n",
    "Podemos tirar os seguintes pontos:\n",
    "\n",
    "- Pode-se observar que há nomes duplicados, pois existem um número grande de redes de restaurantes como, por exemplo, MCDONALDS. \n",
    "\n",
    "- Para analisar se há duplicatas de fato, iremos criar uma df com as condições do nome e endereço serem os mesmos;\n",
    "\n",
    "- Vemos que existem restaurates duplicados que nano são de redes. Assim, necessitamos entender este fato. Iremos criar uma outra df com os filtros de nome duplicado e chain = False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando df com nome e endereço iguais\n",
    "df_add_obj = df_rest[df_rest.duplicated(subset=['object_name', 'address'], keep=False)]\n",
    "\n",
    "# mostrando df\n",
    "df_add_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão Intermediária**\n",
    "\n",
    "Não existem dados que são duplicados no nome e no endereço. Logo iremos analisar os valores que são duplicados no nome e não são de redes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtrar os valores duplicados de object name e chain com valor False\n",
    "df_chain_obj = df_rest[df_rest.duplicated(subset='object_name', keep=False) & (df_rest['chain'] == False)]\n",
    "\n",
    "# exibir o DataFrame df_add_obj\n",
    "df_chain_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão Intermediária**\n",
    "\n",
    "Podemos observar que inicialmente já temos um exemplo que parece um erro. O object_name (nome do restaurante) ADIMIRALS CLUB está duplicado e com dois endereços que parecem ser um ao lado do outro. Provavelmente, isso pode ser devido a erros de digitação ou compilação dos dados.\n",
    "\n",
    "Iremos filtrar a df por nomes dos restaurantes e analisar os endereços."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organizar df\n",
    "df_chain_obj = df_chain_obj.sort_values(by='object_name')\n",
    "\n",
    "# mostrar\n",
    "df_chain_obj.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chain_obj.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão**\n",
    "\n",
    "Como conclusão podemos tirar que:\n",
    "\n",
    "* Existem estabelecimentos que não são de redes, mas que possuem mais de um estabelecimento na base;\n",
    "\n",
    "* Os estabelecimentos duplicados diferenetemente do que imaginado inicialmente parecem não serem duplicados, podemos ver isso pelo endereço e pelo número de assentos para cada um deles.\n",
    "\n",
    "* Vamos manter os estabelecimentos duplicados para a coluna object_name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dados faltantes coluna chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtrando df\n",
    "df_null = df_rest[df_rest.isnull().any(axis=1)]\n",
    "\n",
    "# mostrando df\n",
    "df_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão Intermediária**\n",
    "\n",
    "É possível que tenha acontecido um erro ao compilar os dados de estabelecimentos, assim iremos analisar para entender se os os estabelecimentos possuem outros que possamos substituir os valores NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de restaurantes com chain NaN\n",
    "rest_list = df_null['object_name'].tolist()\n",
    "\n",
    "# Fitlrando na base completa pelos nomes dos restaurantes \n",
    "(df_rest[df_rest['object_name'].isin(rest_list)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão Intermediária**\n",
    "\n",
    "Podemos observar que não existe nenhum outro estabelecimento cadastrado na base recebida. Assim, não podemos ter certeza se os restaurantes são de rede ou não. Assim, iremos decidir por retirar esses dados da análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar as linhas que não estão na lista de nomes a remover\n",
    "df_rest = df_rest[~df_rest['object_name'].isin(rest_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dados duplicados coluna address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando df com nome e endereço iguais\n",
    "df_add = df_rest[df_rest.duplicated(subset=['address'], keep=False)]\n",
    "\n",
    "# mostrando df\n",
    "df_add = df_add.sort_values(by='address')\n",
    "df_add.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão Intermediária**\n",
    "\n",
    "Através dos dados gerados podemos perceber os seguintes fatos:\n",
    "\n",
    "* Existem 2047 dados com o mesmo registro de endereço;\n",
    "\n",
    "* Existem estabelecimentos que estão duplicados no endereço, mas não são o mesmo estabelecimento. Isso ocorre, por exemplo, em shoppings, aeroportos e conglomerados.\n",
    "\n",
    "* Existem, ainda, estabelecimentos duplicados implicitamente (com nomes um pouco diferente) e com mesmo endereço, mas com o a coluna 'number' diferente, um exemplo:\n",
    "    * 7642\t19428\tNATALIE THAI\t998 S ROBERTSON BLVD\tFalse\tRestaurant\t148\n",
    "    * 733\t12519\tNATALEE THAI\t998 S ROBERTSON BLVD\tTrue\tRestaurant\t62\n",
    "    \n",
    "* A primeira hipótese era a de retirada de duplicatas de maneira simples e manual, mas o problema é que existem muitos registros. A segunda era pela padronização pela coluna number, mas percebemos que essa coluna não está padronizada.\n",
    "\n",
    "* A coluna chain apresenta discrepâncias também e podemos observar no exemplo acima dado. Onde claramente os restaurantes parecem ser os mesmos, mas com dados discrepantes.\n",
    "\n",
    "**Takeaways**\n",
    "\n",
    "* Iremos verificar o exemplo do restaurante NATALIE THAI;\n",
    "\n",
    "* Para resolução inicial, iremos implementar um modelo de Machine Learning para limpeza de dados utilizando a biblioteca **recordlinkage** que trata dados pela similaridade. (verificar em: https://recordlinkage.readthedocs.io/en/latest/)\n",
    "\n",
    "* Iremos verificar como foi a limpeza dos dados e realizar o set de parametros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verificando NATALEE THAI\n",
    "df_rest[df_rest['object_name'] == 'NATALEE THAI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verificando NATALIE THAI\n",
    "df_rest[df_rest['object_name'] == 'NATALIE THAI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão Intermediária**\n",
    "\n",
    "* Apesar da similaridade encontrada não podemos afirmar de fato que são os mesmos estabelecimentos. \n",
    "\n",
    "* Podemos ver que existem diferenças entre o tratamento dos dados, por mais bizarro que possa parecer, esse exemplo pode realmente existir. \n",
    "\n",
    "* Para gerar nosso modelo de limpeza de dados iremos considerar a as similaridades nas colunas:\n",
    "    * object_name --> Terá 75% de similaridade no nome\n",
    "    * address --> Terá que ser o mesmo endereço\n",
    "    * chain --> Terá que ser do mesmo tipo (True / False)\n",
    "    * object_type --> Terá que ser do mesmo tipo de estabelecimento\n",
    "    * number --> Terá que ter o mesmo número de de assentos disponíveis\n",
    "    \n",
    "* Faremos esses filtros afim de retirar dados de maneira mais consistente e iremos considerar as possibilidades acima para retirada de duplicatas.\n",
    "\n",
    "* Consideramos 75% como start-point e sabemos que precisamos fazer um trade-off entre a precisão e recall de dados para falsos positivos. Iremos analisar a eficiencia e grau de retirada do script e, se necessário, ajustar. Iremos realizar testes posteriores com 65% - 75% - 85% e decidir qual iremos utilizar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando função para aplicação e análise\n",
    "\n",
    "def limpeza(df,similar):\n",
    "    # Primeiro, criamos um índice para realizar a comparação\n",
    "    indexer = recordlinkage.Index()\n",
    "    indexer.full()\n",
    "    pairs = indexer.index(df)\n",
    "\n",
    "    # Em seguida, comparamos os pares de registros nos campos que nos interessam\n",
    "    compare = recordlinkage.Compare()\n",
    "\n",
    "    compare.string('object_name', 'object_name', method='levenshtein', label='name')\n",
    "    compare.exact('address', 'address', label='address')\n",
    "    compare.exact('chain', 'chain', label='chain')\n",
    "    compare.exact('object_type', 'object_type', label='object_type')\n",
    "    compare.exact('number', 'number', label='number')\n",
    "\n",
    "    # Calculamos a similaridade\n",
    "    features = compare.compute(pairs, df)\n",
    "\n",
    "    # Aqui, vamos dizer que dois registros são duplicatas se todos os critérios forem atendidos\n",
    "    matches = features[(features['name'] > similar) & (features['address'] == 1.0) & (features['chain'] == 1.0) & (features['object_type'] == 1.0) & (features['number'] == 1.0)]\n",
    "\n",
    "    # Recuperamos os índices dos registros duplicados\n",
    "    duplicate_indices = matches.index.get_level_values(1)\n",
    "\n",
    "    # Removemos as duplicatas\n",
    "    df = df[~df.index.isin(duplicate_indices)]\n",
    "    \n",
    "    # Info da df nova\n",
    "    df = df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# informação df_add\n",
    "df_add.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplicando a função e analisando - 0.65 de precisão\n",
    "limpeza(df_add,0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplicando a função e analisando - 0.55 de precisão\n",
    "limpeza(df_add,0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplicando a função e analisando - 0.75 de precisão\n",
    "limpeza(df_add,0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplicando a função e analisando - 0.85 de precisão\n",
    "limpeza(df_add,0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão**\n",
    "\n",
    "A partir da limpeza realizada na base df_add que é a base de dados com endereços duplicados e utilizando o script com recordlinkage podemos perceber que a diferença para os parametros citados é ínfima, ou seja, com os nossos sets iniciais não se encontrou muitas duplicadas.\n",
    "\n",
    "Iremos considerar para limpeza da base de dados o parâmetro com 0,55 de similaridade e com os outros padrões das outras colunas para uma limpeza na base geral de dados (df_rest). Iremos perder um número mínimo de dados.\n",
    "\n",
    "Assim, a partir de nossas premissas, iremos limpar os dados e considerar que estes dados estão corretos e não duplicados apesar de nomes similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Script para limpeza de dados\n",
    "\n",
    "# Primeiro, criamos um índice para realizar a comparação\n",
    "indexer = recordlinkage.Index()\n",
    "indexer.full()\n",
    "pairs = indexer.index(df_rest)\n",
    "\n",
    "# Em seguida, comparamos os pares de registros nos campos que nos interessam\n",
    "compare = recordlinkage.Compare()\n",
    "\n",
    "compare.string('object_name', 'object_name', method='levenshtein', label='name')\n",
    "compare.exact('address', 'address', label='address')\n",
    "compare.exact('chain', 'chain', label='chain')\n",
    "compare.exact('object_type', 'object_type', label='object_type')\n",
    "compare.exact('number', 'number', label='number')\n",
    "\n",
    "# Calculamos a similaridade\n",
    "features = compare.compute(pairs, df_rest)\n",
    "\n",
    "# Aqui, vamos dizer que dois registros são duplicatas se todos os critérios forem atendidos\n",
    "matches = features[(features['name'] > 0.55) & (features['address'] == 1.0) & (features['chain'] == 1.0) & (features['object_type'] == 1.0) & (features['number'] == 1.0)]\n",
    "\n",
    "# Recuperamos os índices dos registros duplicados\n",
    "duplicate_indices = matches.index.get_level_values(1)\n",
    "\n",
    "# Removemos as duplicatas\n",
    "df_rest = df_rest[~df_rest.index.isin(duplicate_indices)]\n",
    "    \n",
    "# Info da df nova\n",
    "df_rest.info()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**\n",
    "\n",
    "Ao aplicar o código a Dataframe inteira o código acaba matando o Kernel. Iremos realizar o seguinte passo:\n",
    "\n",
    "* Separar a df entre dados com endereço duplicado e não duplicado;\n",
    "* aplicar o script na base com endereço duplicado;\n",
    "* juntar os dados posteriormente em uma nova base;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separação dos dados\n",
    "duplicated_addresses = df_rest[df_rest.duplicated(subset='address', keep=False)]\n",
    "df_duplicated = df_rest[df_rest['address'].isin(duplicated_addresses['address'])]\n",
    "df_not_duplicated = df_rest[~df_rest['address'].isin(duplicated_addresses['address'])]\n",
    "\n",
    "# Script de limpeza de dados\n",
    "indexer = recordlinkage.Index()\n",
    "indexer.full()\n",
    "pairs = indexer.index(df_duplicated)\n",
    "\n",
    "compare = recordlinkage.Compare()\n",
    "compare.string('object_name', 'object_name', method='levenshtein', label='name')\n",
    "compare.exact('address', 'address', label='address')\n",
    "compare.exact('chain', 'chain', label='chain')\n",
    "compare.exact('object_type', 'object_type', label='object_type')\n",
    "compare.exact('number', 'number', label='number')\n",
    "\n",
    "features = compare.compute(pairs, df_duplicated)\n",
    "matches = features[(features['name'] > 0.55) & (features['address'] == 1.0) & (features['chain'] == 1.0) & (features['object_type'] == 1.0) & (features['number'] == 1.0)]\n",
    "duplicate_indices = matches.index.get_level_values(1)\n",
    "df_duplicated = df_duplicated[~df_duplicated.index.isin(duplicate_indices)]\n",
    "\n",
    "# Juntar os DataFrames\n",
    "df_rest = pd.concat([df_not_duplicated, df_duplicated])\n",
    "\n",
    "# Informação sobre a nova DataFrame\n",
    "df_rest.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão**\n",
    "\n",
    "Como conclusão da limpeza de dados podemos tirar que não perdemos muitos dados das limpezas que foram feitas. Os parametros utilizados de similaridade de nome, mesmo endereço e mesmo parametros de chain, tipo de estabelecimento e número de mesas filtraram os dados para mostrar se há ou não uma chance real de serem os mesmos estabelecimentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA - Base de dados\n",
    "\n",
    "Iremos realizar uma análise estatística de dados para a base de dados para todas as colunas e tirar conclusões iniciais acerca delas. \n",
    "\n",
    "Iremos análisar:\n",
    "\n",
    "- Número diferentes de restaurantes coletados\n",
    "- Número diferentes de endereços coletados\n",
    "- Número de redes e número de não redes\n",
    "- Distribuição dos tipos de estabelecimentos\n",
    "- Distribuição dos número de assentos --> Podemos ter outliers ou dados a retirar;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EDA - Número de restaurantes da base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número diferentes de restaurantes\n",
    "df_rest['object_name'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EDA - Diferentes endereços"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rest['address'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EDA - Distribuição entre número de redes e não redes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar o número de valores únicos de \"object_name\"\n",
    "unique_object_names = df_rest['object_name'].nunique()\n",
    "\n",
    "# Contar quantos são de rede (\"chain\" é True) e quantos não são de rede (\"chain\" é False)\n",
    "chain_counts = df_rest['chain'].value_counts()\n",
    "num_chain_true = chain_counts[True]\n",
    "num_chain_false = chain_counts[False]\n",
    "\n",
    "# Plotar um gráfico de pizza para mostrar a distribuição\n",
    "labels = ['Rede', 'Não Rede']\n",
    "values = [num_chain_true, num_chain_false]\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values)])\n",
    "fig.update_layout(title=f\"Distribuição de Rede vs. Não Rede para {unique_object_names} valores únicos de 'object_name'\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EDA - Distribuição dos tipos de estabelecimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover duplicados dos nomes\n",
    "df_unique_names = df_rest.drop_duplicates(subset='object_name')\n",
    "\n",
    "# Categorizar e contar o número de valores diferentes em \"object_type\"\n",
    "object_type_counts = df_unique_names['object_type'].value_counts()\n",
    "\n",
    "# Obter a lista de cores disponíveis na paleta Plotly\n",
    "colors = plotly.colors.DEFAULT_PLOTLY_COLORS\n",
    "\n",
    "# Plotar um gráfico de barras com cores diferentes para cada classe\n",
    "fig = go.Figure(data=[go.Bar(\n",
    "    x=object_type_counts.index,\n",
    "    y=object_type_counts.values,\n",
    "    marker=dict(color=colors[:len(object_type_counts)])\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Número de estabelecimentos únicos na região de Los Angeles',\n",
    "    xaxis_title='Tipo',\n",
    "    yaxis_title='Contagem'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EDA - Distribuição dos número de assentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um objeto de subplot\n",
    "fig = sp.make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1)\n",
    "\n",
    "# Adicionar o histograma à primeira linha do subplot\n",
    "histogram_trace = go.Histogram(x=df_rest['number'], nbinsx=20)\n",
    "fig.add_trace(histogram_trace, row=1, col=1)\n",
    "\n",
    "# Adicionar o gráfico de distribuição à segunda linha do subplot\n",
    "distribution_trace = go.Histogram(x=df_rest['number'], histnorm='probability')\n",
    "fig.add_trace(distribution_trace, row=2, col=1)\n",
    "\n",
    "# Atualizar o layout do subplot\n",
    "fig.update_layout(\n",
    "    title='Distribuição e Histograma da coluna \"number\"',\n",
    "    xaxis_title='Número',\n",
    "    yaxis_title='Contagem',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Mostrar o subplot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o boxplot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Adicionar o boxplot à figura\n",
    "fig.add_trace(go.Box(y=df_rest['number'], name='Número'))\n",
    "\n",
    "# Atualizar o layout da figura\n",
    "fig.update_layout(\n",
    "    title='Boxplot da coluna \"number\"',\n",
    "    xaxis_title='Coluna',\n",
    "    yaxis_title='Número',\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        x=0.05,\n",
    "        y=0.95,\n",
    "        bordercolor='gray',\n",
    "        borderwidth=1\n",
    "    ),\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white'\n",
    ")\n",
    "\n",
    "# Mostrar o boxplot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão**\n",
    "\n",
    "Podemos tirar as seguintes conclusões:\n",
    "\n",
    "- Há 8665 restaurantes diferentes em nossa análise;\n",
    "\n",
    "- Estamos avaliando 8514 endereços diferentes\n",
    "\n",
    "- 62% não são de redes e 38% dos restaurantes analisados são de redes\n",
    "\n",
    "- A maior parte são classificados como restaurantes\n",
    "\n",
    "- A distribuição dos assentos não são normais, existem muitos restaurantes até 59 assentos após isso o número diminui drasticamente;\n",
    "\n",
    "- Não existem erros, o restaurante com menor número de assentos é de 1 assento e o maior possuí 229. A mediana fica em 27 assentos. Iremos utilizar a mediana, pois demonstra de maneira melhor a centralidade dos dados.\n",
    "\n",
    "\n",
    "Agora que analisamos e observamos as especificações analíticas dos dados iremos gerar nossos gráficos e análises. Não iremos retirar os restaurantes com 1 assento, pois podem ser restaurantes exclusivos ou personalizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise dos dados\n",
    "\n",
    "Iremos gerar as seguintes análises:\n",
    "\n",
    "* Investigaremos as proporções de vários tipos de estabelecimentos e construiremos um gráfico.\n",
    "* Investigaremos as proporções de estabelecimentos de rede e não de rede e construiremos um gráfico.\n",
    "* Descobriremos qual tipo de estabelecimento é típico para redes.\n",
    "* Analisaremos o que caracteriza redes: se são muitos estabelecimentos com um pequeno número de assentos ou poucos estabelecimentos com muitos assentos.\n",
    "* Determinaremos o número médio de assentos para cada tipo de restaurante. Em média, qual tipo de restaurante terá o maior número de assentos? Construiremos gráficos para visualizar essas informações.\n",
    "* Colocaremos os dados dos nomes das ruas da coluna \"address\" em uma coluna separada.\n",
    "* Construiremos um gráfico com as dez ruas que terão o maior número de restaurantes.\n",
    "* Encontraremos o número de ruas que terão apenas um restaurante.\n",
    "* Para as ruas com muitos restaurantes, analisaremos a distribuição do número de assentos e quais tendências conseguiremos observar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proporções de vários tipos de estabelecimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando análise e gráficos \n",
    "object_types = df_rest['object_type'].value_counts(normalize=True)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=object_types.index, y=object_types.values)\n",
    "plt.title('Proporção de Tipos de Estabelecimentos')\n",
    "plt.xlabel('Tipo de Estabelecimento')\n",
    "plt.ylabel('Proporção')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão**\n",
    "\n",
    "Como vimos anteriormente e seguindo o mesmo padrão temos os principais estabelcimeentos classficados como restaurantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Comentário do revisor v1</strong>\n",
    "\n",
    "Perfeito\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proporções de estabelecimentos de rede e não:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando análise\n",
    "chain_counts = df_rest['chain'].value_counts(normalize=True)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(chain_counts, labels=['Rede' if is_chain else 'Não rede' for is_chain in chain_counts.index], autopct='%1.1f%%')\n",
    "plt.title('Proporção de Estabelecimentos de Rede vs Não Rede')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão**\n",
    "\n",
    "Já havíamos gerado essa conclusão e gráfico em nossa análise de EDA, mas como podemos ver há uma volume muito maior para estabelecimentos que não são de rede. Aproximadamente 62% não são de rede e 38% são de redes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Comentário do revisor v1</strong>\n",
    "\n",
    "Correto!\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipo de estabelecimento típico para redes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculando e gerando gráficos para redes\n",
    "chain_types = df_rest[df_rest['chain'] == True]['object_type'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "sns.barplot(x=chain_types.index, y=chain_types.values)\n",
    "plt.title('Tipos de Estabelecimentos em Redes')\n",
    "plt.xlabel('Tipo de Estabelecimento')\n",
    "plt.ylabel('Quantidade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de redes\n",
    "df_rest[df_rest['chain'] == True]['object_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de não redes\n",
    "df_rest[df_rest['chain'] == False]['object_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculando e gerando gráficos para Não redes\n",
    "chain_types = df_rest[df_rest['chain'] == False]['object_type'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "sns.barplot(x=chain_types.index, y=chain_types.values)\n",
    "plt.title('Tipos de Estabelecimentos em Não Redes')\n",
    "plt.xlabel('Tipo de Estabelecimento')\n",
    "plt.ylabel('Quantidade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão**\n",
    "\n",
    "Como podemos observar e em comparação a análise geral da base:\n",
    "\n",
    "- Os restaurantes ainda continuam como top1;\n",
    "\n",
    "- O fast food ainda é o segundo;\n",
    "\n",
    "- Para redes as padarias crescem de maneira que sai de último para terceiro;\n",
    "\n",
    "- Os cafés continuam de maneira incidente;\n",
    "\n",
    "- Os bares não são tão incidentes para redes;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos gerar análises para clusterizada para cada grupo em % para o tipo de estabelecimento para cada tipo para análise de rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando a quantidade de cada tipo de estabelecimento que é de rede\n",
    "chain_types = df_rest[df_rest['chain'] == True]['object_type'].value_counts()\n",
    "\n",
    "# Calculando a quantidade total de cada tipo de estabelecimento\n",
    "total_types = df_rest['object_type'].value_counts()\n",
    "\n",
    "# Calculando a porcentagem de cada tipo de estabelecimento que é de rede\n",
    "chain_percentage = (chain_types / total_types) * 100\n",
    "\n",
    "# Plotando o gráfico\n",
    "plt.figure(figsize=(16,9))\n",
    "sns.barplot(x=chain_percentage.index, y=chain_percentage.values)\n",
    "plt.title('Porcentagem de Estabelecimentos em Redes por Tipo')\n",
    "plt.xlabel('Tipo de Estabelecimento')\n",
    "plt.ylabel('Porcentagem (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão**\n",
    "\n",
    "Como conclusão podemos tirar que:\n",
    "\n",
    "* A partir dos dados gerados em ambos os tipos, ao parametrizar os estabelecimetnos por percentual a gente pode observar que padarias e cafés tem os maiores percentual para redes de estabelecimentos.\n",
    "\n",
    "* As padarias atingem quase 100% do valor total.\n",
    "\n",
    "* Os cafés tem um total de, aproximadamente, 60% isso demonstra que o mercado de café tem grande percentaual de redes de café.\n",
    "\n",
    "* Ao observar a tendência para as cafeterias serem de redes a gente pode entender que um dos pontos é que este mercado é altamente padronizada, ou seja, sabemos que o mercado de redes funcionam de maneira padronizada e não oferecem diferentes experiências para o cliente, isso é um fato muito interessante, pois apresenta um potencial competitivo e diferencial alto para nosso modelo de negócio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que caracteriza redes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando descrição para redes\n",
    "# Criar o boxplot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Adicionar o boxplot à figura\n",
    "fig.add_trace(go.Box(y=df_rest[df_rest['chain'] == True]['number'], name='Número'))\n",
    "\n",
    "# Atualizar o layout da figura\n",
    "fig.update_layout(\n",
    "    title='Boxplot da coluna \"number para redes\"',\n",
    "    xaxis_title='Coluna',\n",
    "    yaxis_title='Número',\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        x=0.05,\n",
    "        y=0.95,\n",
    "        bordercolor='gray',\n",
    "        borderwidth=1\n",
    "    ),\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white'\n",
    ")\n",
    "\n",
    "# Mostrar o boxplot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando descrição para não redes\n",
    "# Criar o boxplot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Adicionar o boxplot à figura\n",
    "fig.add_trace(go.Box(y=df_rest[df_rest['chain'] == False]['number'], name='Número'))\n",
    "\n",
    "# Atualizar o layout da figura\n",
    "fig.update_layout(\n",
    "    title='Boxplot da coluna \"number para não redes\"',\n",
    "    xaxis_title='Coluna',\n",
    "    yaxis_title='Número',\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        x=0.05,\n",
    "        y=0.95,\n",
    "        bordercolor='gray',\n",
    "        borderwidth=1\n",
    "    ),\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white'\n",
    ")\n",
    "\n",
    "# Mostrar o boxplot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão**\n",
    "\n",
    "- Podemos perceber que a mediana é de 25 cadeiras para os estabelecimentos de rede;\n",
    "\n",
    "- Há um grande desvio padrão, demonstrando que existem vários tamanhos de estabelecimetnos para as redes também. \n",
    "\n",
    "- Existem estabelecimentos com o mínimo de assentos possíveis, contando somente com 1, podem ser consideradas redes personalizadas;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Número médio de assentos para cada tipo de estabelecimento:\n",
    "\n",
    "Iremos utilizar o número mediano para representar melhor o valor central dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando análise e gráfico\n",
    "avg_seats = df_rest.groupby('object_type')['number'].median()\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "sns.barplot(x=avg_seats.index, y=avg_seats.values)\n",
    "plt.title('Número Médio de Assentos por Tipo de Estabelecimento')\n",
    "plt.xlabel('Tipo de Estabelecimento')\n",
    "plt.ylabel('Número Médio de Assentos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostrando os valores quantitativos\n",
    "avg_seats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão**\n",
    "\n",
    "Pode-se perceber que bares e restaurantes tem os maiores números de assentos. Padarias o menor número, seguidos de pizzarias e fast foods e cafés.\n",
    "\n",
    "Esses dados fazem sentido, pois se formos pensar estabelecimentos que os clientes passam mais tempo dentro precisam de maior número de assentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de estabelecimentos por nome da rua\n",
    "\n",
    "Iremos gerar algumas análises e steps:\n",
    "\n",
    "- Colocar os dados dos nomes das ruas da coluna address em uma coluna separada\n",
    "\n",
    "- Gerar as Dez ruas com o maior número de restaurantes\n",
    "\n",
    "- Compilar o Número de ruas que têm apenas um restaurante\n",
    "\n",
    "- enteder a istribuição de número de assentos para as ruas com muitos restaurantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colocar os dados dos nomes das ruas da coluna address em uma coluna separada\n",
    "df_rest['street'] = df_rest['address'].apply(lambda x: re.sub(r'[0-9#]', '', x))\n",
    "df_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar as Dez ruas com o maior número de restaurantes\n",
    "top_streets = df_rest['street'].value_counts().head(10)\n",
    "\n",
    "# gerando imagem\n",
    "plt.figure(figsize=(16,9))\n",
    "sns.barplot(x=top_streets.index, y=top_streets.values)\n",
    "plt.title('Top 10 Ruas com Mais Estabelecimentos')\n",
    "plt.xlabel('Rua')\n",
    "plt.ylabel('Quantidade de Estabelecimentos')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruas com único restaurante\n",
    "single_restaurant_streets = df_rest['street'].value_counts()\n",
    "num_single_restaurant_streets = (single_restaurant_streets == 1).sum()\n",
    "print('Número de ruas que têm apenas um restaurante: ', num_single_restaurant_streets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# número de ruas\n",
    "df_rest['street'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuição de número de assentos para as ruas com muitos restaurantes:\n",
    "# Primeiro, obtemos uma lista das 10 ruas com mais restaurantes.\n",
    "top_streets_list = top_streets.index.tolist()\n",
    "\n",
    "# Em seguida, criamos um novo DataFrame que contém apenas os restaurantes localizados nessas 10 ruas.\n",
    "# Fazemos isso filtrando o DataFrame original com base em se a rua de cada restaurante está na nossa lista das top 10 ruas.\n",
    "df_top_streets = df_rest[df_rest['street'].isin(top_streets_list)]\n",
    "\n",
    "# Agora, criamos um boxplot (gráfico de caixa) que mostra a distribuição do número de assentos para cada uma dessas ruas.\n",
    "# Para fazer isso, usamos a biblioteca Seaborn e fornecemos a rua (x) e o número de assentos (y) como inputs.\n",
    "# O parâmetro data é usado para especificar o DataFrame que estamos usando.\n",
    "plt.figure(figsize=(16,9))\n",
    "sns.boxplot(x='street', y='number', data=df_top_streets)\n",
    "\n",
    "# Por fim, adicionamos um título ao gráfico e rotulamos os eixos para maior clareza.\n",
    "plt.title('Distribuição de Número de Assentos para Ruas com Muitos Restaurantes')\n",
    "plt.xlabel('Rua')\n",
    "plt.ylabel('Número de Assentos')\n",
    "\n",
    "# Como o nome das ruas pode ser longo e complexo, rotacionamos os rótulos do eixo x em 90 graus para melhor visualização.\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão**\n",
    "\n",
    "Pode-se perceber que a WILSHIRE BLVD é a rua com o maior número de assentos por ruas, seguida da HOLLYWOOD BLVD E W SUNSET BLV. \n",
    "\n",
    "Apesar disso, a rua com maior número de estabelecimentos W THT ST parece contar com menor número de assentos. Isso pode estar ligado a rotatividade por região, sendo que a W THT deve ter clientes em maior Rush diário e menos sentados.\n",
    "\n",
    "As médias de assentos ficaram muito parecidas para todos os casos, apesar de um alto desvio padrão positivio para o número de assentos nos endereços."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão Final e Pitch Deck\n",
    "\n",
    "\n",
    "**Acerca da análise**\n",
    "\n",
    "Após uma análise detalhada, concluímos que existem algumas inconsistências e discrepâncias nos dados, especialmente em relação a duplicatas, dados faltantes e problemas com padronização. Observamos que, embora existam nomes de restaurantes duplicados, isso se deve principalmente à presença de cadeias de restaurantes, como o McDonald's.\n",
    "\n",
    "Apesar dessas duplicatas, após uma análise mais aprofundada, descobrimos que muitos desses casos não são de fato duplicatas. Por exemplo, alguns restaurantes com o mesmo nome, mas com endereços ligeiramente diferentes, podem ser devido a erros de digitação ou problemas na compilação dos dados, mas também podem representar estabelecimentos legítimos e separados.\n",
    "\n",
    "Na tentativa de corrigir as duplicatas, inicialmente pensamos em eliminar as duplicatas manualmente ou através de padronização, mas logo percebemos que isso não seria viável devido ao grande volume de dados. Em vez disso, decidimos implementar um modelo de Machine Learning para limpeza de dados, utilizando a biblioteca recordlinkage, que trata dados pela similaridade.\n",
    "\n",
    "Nosso modelo considera vários fatores ao determinar a similaridade entre as entradas, incluindo o nome do estabelecimento, o endereço, se o estabelecimento é parte de uma rede, o tipo de estabelecimento, e o número de assentos disponíveis. Através desse método, conseguimos limpar a base de dados e remover duplicatas de forma eficiente e precisa.\n",
    "\n",
    "Em relação aos nossos insights analíticos, concluímos que:\n",
    "\n",
    "* Temos um total de 8665 restaurantes diferentes e 8514 endereços diferentes na nossa base de dados.\n",
    "* A maioria dos restaurantes (62%) não faz parte de uma rede, enquanto 38% fazem.\n",
    "* A distribuição de assentos nos restaurantes não é normal, com a maioria dos restaurantes tendo até 59 assentos. A mediana é de 27 assentos.\n",
    "* As ruas WILSHIRE BLVD, HOLLYWOOD BLVD e W SUNSET BLVD possuem o maior número de assentos por ruas, sugerindo que podem ser áreas populares para restaurantes maiores ou com maior capacidade de atendimento.\n",
    "* Apesar de W THT ST ter o maior número de estabelecimentos, apresenta um número menor de assentos, o que pode indicar uma alta rotatividade de clientes.\n",
    "\n",
    "\n",
    "Além disso, com base na análise dos dados, podemos concluir que:\n",
    "\n",
    "* A partir dos dados gerados em ambos os tipos, ao parametrizar os estabelecimentos por percentual, observamos que padarias e cafés têm o maior percentual de pertencer a redes de estabelecimentos.\n",
    "* As padarias atingem quase 100% do valor total, enquanto os cafés apresentam um total de aproximadamente 60%. Isso demonstra que o mercado de café tem um grande percentual de redes de cafés.\n",
    "* Ao observar a tendência para as cafeterias serem de redes, podemos entender que este mercado é altamente padronizado, ou seja, sabemos que o mercado de redes funciona de maneira padronizada e não oferece diferentes experiências para o cliente. Isso é um fato muito interessante, pois apresenta um potencial competitivo e diferencial alto para nosso modelo de negócio.\n",
    "\n",
    "\n",
    "Em conclusão, embora tenhamos enfrentado alguns desafios na limpeza e na análise dos dados, conseguimos obter uma visão detalhada e precisa do panorama da indústria de restaurantes em nossa área de estudo. A partir disso, somos capazes de prosseguir com a produção de gráficos e análises mais detalhados.\n",
    "\n",
    "\n",
    "\n",
    "**Pitch Deck e apresentação para investidores**\n",
    "\n",
    "Com a análise de mercado realizada, identificamos que o segmento de cafeterias em Los Angeles, apesar de já estabelecido, apresenta oportunidades consideráveis para inovação. Observamos a predominância de estabelecimentos que não são parte de redes e, com isso, surge o potencial de criação de uma experiência única e personalizada para os clientes.\n",
    "\n",
    "A implementação de garçons robôs em nossa cafeteria permitirá superar os desafios que o mercado enfrenta atualmente, como filas exaustivas nos horários de pico, limitações de personalização do menu e a questão da mão de obra escassa e por vezes não qualificada.\n",
    "\n",
    "A nossa localização na W THT Street, uma rua com alto fluxo de pessoas e grande visibilidade, juntamente com a oferta de um maior número deassentos em comparação à média do mercado, nos coloca em uma posição estratégica para atrair e reter um público diversificado.\n",
    "\n",
    "Ainda que sejamos um estabelecimento independente, isso nos possibilita menor custo inicial de implementação, maior liberdade para a personalização e criação da identidade da marca, além de nos permitir oferecer uma experiência única e futurista aos nossos clientes.\n",
    "\n",
    "Ao analisar o mercado, identificamos que a proposta de nossa cafeteria com garçons robôs tem potencial para revolucionar a indústria na cidade, oferecendo um serviço eficiente e personalizado, além de contribuir para a elevação do padrão de serviço e experiência dos clientes no segmento de cafeterias.\n",
    "\n",
    "A observação da tendência para as cafeterias serem de redes e a padronização resultante oferece uma oportunidade significativa para nós. Como nosso modelo de negócios se concentra em proporcionar uma experiência única e personalizada aos clientes, temos a chance de se destacar e competir efetivamente no mercado.\n",
    "\n",
    "Dessa forma, temos a combinação ideal entre localização, inovação tecnológica e estratégia de negócios, prontos para nos estabelecer como referência em atendimento eficiente e personalizado em cafeterias em Los Angeles. A análise de mercado reforça nossa confiança de que nossa proposta de negócio é não apenas viável, mas tem um grande potencial para sucesso e crescimento.\n",
    "\n",
    "\n",
    "**Pitch Deck**\n",
    "https://drive.google.com/file/d/1A_UkNovv_WrH0Yycaiv3b1l8o6pbfbhe/view?usp=share_link\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "236px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
